{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 09:41:22.842080 140409792354048 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import read\n",
    "import prepare_data\n",
    "import input_builder\n",
    "\n",
    "import importlib\n",
    "import model\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_SEQ_LENGTH = 128\n",
    "\n",
    "importlib.reload(prepare_data)\n",
    "import os\n",
    "os.environ['TFHUB_CACHE_DIR'] = '/home/djjindal/bert/script-learning'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "model_dir = 'output_triple'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<input_builder.InputFeatures object at 0x7fb317c8cbe0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7874"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = 'dataset/gw_extractions_no_rep_no_fin.pickle'\n",
    "train_dataset = read.read_data_iterator(dataset)\n",
    "features = list(prepare_data.tokenize_if_small_enough(train_dataset,sentences=False,no_context=False))\n",
    "train_set = features[:int(0.8 * len(features))]\n",
    "val_set = features[int(0.8 * len(features)):int(0.9*len(features))]\n",
    "print(train_set[0])\n",
    "len(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'output_triple', '_tf_random_seed': None, '_save_summary_steps': 0, '_save_checkpoints_steps': 0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb315f67630>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 09:41:36.284496 140409792354048 tf_logging.py:115] Using config: {'_model_dir': 'output_triple', '_tf_random_seed': None, '_save_summary_steps': 0, '_save_checkpoints_steps': 0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb315f67630>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=model_dir,\n",
    "    save_summary_steps=0,\n",
    "    save_checkpoints_steps=0,\n",
    "    log_step_count_steps=100)\n",
    "\n",
    "model_fn = model.model_fn_builder(\n",
    "  num_labels=5,\n",
    "  learning_rate=LEARNING_RATE,\n",
    "  num_train_steps=1,\n",
    "  num_warmup_steps=1)\n",
    "\n",
    "train_test_input_fn = input_builder.input_fn_builder(\n",
    "    features=train_set,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False,\n",
    "    candidates=5)\n",
    "\n",
    "val_test_input_fn = input_builder.input_fn_builder(\n",
    "    features=val_set,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False,\n",
    "    candidates=5)\n",
    "\n",
    "estimator = tf.estimator.Estimator(\n",
    "  model_fn=model_fn,\n",
    "  config=run_config,\n",
    "  params={\"batch_size\": BATCH_SIZE})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICT FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def predict(sentenecs, triples, candidates, entity):\n",
    "    e_dict = dict()\n",
    "    check_dataset = []\n",
    "    e_dict['sentences'] = sentenecs\n",
    "    e_dict['triples'] = triples\n",
    "    e_dict['candidates'] = candidates\n",
    "    e_dict['correct'] = 4\n",
    "    e_dict['entity'] = entity\n",
    "    check_dataset = [e_dict]\n",
    "    predict_set = list(prepare_data.tokenize_if_small_enough(check_dataset, sentences=False,no_context=False))\n",
    "\n",
    "    predict_input_fn = input_builder.input_fn_builder(\n",
    "        features=predict_set,\n",
    "        seq_length=MAX_SEQ_LENGTH,\n",
    "        is_training=False,\n",
    "        drop_remainder=False,\n",
    "        candidates=5)\n",
    "    predictions = estimator.predict(input_fn=predict_input_fn,yield_single_examples=False,no_context=False)\n",
    "\n",
    "    for (i, prediction) in enumerate(predictions):\n",
    "        arr = prediction['probabilities'][0]\n",
    "        prob = [np.exp(arr[0]),np.exp(arr[1]),np.exp(arr[2]),np.exp(arr[3]),np.exp(arr[4])]\n",
    "        total = np.sum(prob)\n",
    "        max= np.max(prob)\n",
    "        ec_dict = check_dataset[i]\n",
    "        print(\"SENTENCE EVENT CHAIN\", ec_dict['sentences'],\"\\n\")\n",
    "        print(\"TRIPLE EVENT CHAIN\", ec_dict['triples'],\"\\n\")\n",
    "        print(\"CANDIDATES\", ec_dict['candidates'],\"\\n\")\n",
    "        print(\"ENTITY\", ec_dict['entity'],\"\\n\")\n",
    "        print(\"PREDICTION\", prediction['labels'], \"with Probability\", np.max(prob),\"\\n\",\"\\n\")\n",
    "        return prediction['labels']\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    print(text1.value, text2.value, text3.value)\n",
    "    print(predict([], text1.value.split('), ('), text2.value.split('), ('), text3.value))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djjindal/venvs/TF-1.3/lib/python3.6/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "213fba9948704717a7d6e5eaadb8c72f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Event Chain')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9bdbc4bca54106becb17670b0edb7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Candidates')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb88e721c13247a2a310ef6ff255dc38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Entity')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86abe73c80fc41adbb1e41feff98c1de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Predict', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import *\n",
    "from IPython.display import display\n",
    "from IPython.html import widgets\n",
    "text1 = widgets.Text(description=\"Event Chain\", width=200)\n",
    "text2 = widgets.Text(description=\"Candidates\", width=200)\n",
    "text3 = widgets.Text(description=\"Entity\", width=200)\n",
    "button = widgets.Button(description=\"Predict\")\n",
    "button.on_click(on_button_clicked)\n",
    "display(text1)\n",
    "display(text2)\n",
    "display(text3)\n",
    "display(button)\n",
    "\n",
    "# [('john','ordered',None),('john','ate','food')]\n",
    "# [('john','left',None),('john','stays',None)]\n",
    "\n",
    "# [('john','ordered',None),('john','paid',None),('john','ate','food')]\n",
    "# [('john','left',None),('john','stays',None)]\n",
    "\n",
    "# [(None, 'took', 'phone call'), (None, 'had', 'something'), (None, 'began', 'to squint')] \n",
    "# [(None, 'broadcast', '(the two previous record-holders'), ('the pennant', 'slammed', None), (None, \"n't imagine\", 'Maris'), (None, 'expressed', 'interest'), (None, 'turned', '68 _')] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djjindal/venvs/TF-1.3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: generator 'read_data_iterator' raised StopIteration\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "I0422 01:01:54.872077 140554432849664 tf_logging.py:115] Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 01:01:58.055485 140554432849664 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 01:01:58.730752 140554432849664 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 01:01:59.852343 140554432849664 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 01:02:00.588213 140554432849664 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 01:02:01.811433 140554432849664 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 01:02:01.997414 140554432849664 tf_logging.py:115] Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 01:02:02.507045 140554432849664 tf_logging.py:115] Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from output_triple/model.ckpt-4800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 01:02:02.511561 140554432849664 tf_logging.py:115] Restoring parameters from output_triple/model.ckpt-4800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 01:02:04.040912 140554432849664 tf_logging.py:115] Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 01:02:04.181905 140554432849664 tf_logging.py:115] Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE EVENT CHAIN [\"``We begin this session with a national spotlight on us,'' he told\\nlawmakers.\", 'Bush also took a a cue from Washington, where presidents often\\nintroduce special guests during their State of the Union speeches.\\n', 'Bush introduced ninth-grader Adam Gonzales of Lubbock as a student\\nwho overcame years of social promotion that put him on an\\neducational course to nowhere.\\n\\n\\n'] \n",
      "\n",
      "TRIPLE EVENT CHAIN [(None, 'told', 'lawmakers'), (None, 'took', 'cue'), (None, 'introduced', 'Gonzales')] \n",
      "\n",
      "CANDIDATES [(None, 'introduced', 'Burgin'), (None, 'can offer', 'campaign-trail promises'), ('all the people', 'expanding', None), (None, 'echoing', 'light'), (None, 'will lay', 'chairman')] \n",
      "\n",
      "ENTITY Bush \n",
      "\n",
      "CORRECT 1 \n",
      "\n",
      "PREDICTION 2 with Probability 0.999869 \n",
      " \n",
      "\n",
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 01:02:06.829935 140554432849664 tf_logging.py:115] Calling model_fn.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1c8f2d35f31b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredict_input_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myield_single_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'probabilities'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/TF-1.3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input_fn, predict_keys, hooks, checkpoint_path, yield_single_examples)\u001b[0m\n\u001b[1;32m    575\u001b[0m             input_fn, model_fn_lib.ModeKeys.PREDICT)\n\u001b[1;32m    576\u001b[0m         estimator_spec = self._call_model_fn(\n\u001b[0;32m--> 577\u001b[0;31m             features, None, model_fn_lib.ModeKeys.PREDICT, self.config)\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0;31m# Call to warm_start has to be after model_fn is called.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/TF-1.3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bert/script-learning/model.py\u001b[0m in \u001b[0;36mmodel_fn\u001b[0;34m(features, labels, mode, params)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m       (predicted_labels, log_probs) = create_model3(\n\u001b[0;32m--> 119\u001b[0;31m         is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m       predictions = {\n",
      "\u001b[0;32m~/bert/script-learning/model.py\u001b[0m in \u001b[0;36mcreate_model3\u001b[0;34m(is_predicting, input_ids, input_mask, segment_ids, labels, num_labels)\u001b[0m\n\u001b[1;32m     38\u001b[0m           \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbert_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m           \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tokens\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m           as_dict=True)\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# Use \"pooled_output\" for classification tasks on an entire sentence.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/TF-1.3/lib/python3.6/site-packages/tensorflow_hub/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, _sentinel, signature, as_dict)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0minput_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_prepare_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/TF-1.3/lib/python3.6/site-packages/tensorflow_hub/native_module.py\u001b[0m in \u001b[0;36mcreate_apply_graph\u001b[0;34m(self, signature, input_tensors, name)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mmeta_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0minput_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         import_scope=relative_scope_name)\n\u001b[0m\u001b[1;32m    548\u001b[0m     fix_colocation_after_import(input_map=feed_map,\n\u001b[1;32m    549\u001b[0m                                 absolute_import_scope=absolute_scope_name)\n",
      "\u001b[0;32m~/venvs/TF-1.3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mimport_meta_graph\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[1;32m   1672\u001b[0m   \"\"\"  # pylint: disable=g-doc-exception\n\u001b[1;32m   1673\u001b[0m   return _import_meta_graph_with_return_elements(\n\u001b[0;32m-> 1674\u001b[0;31m       meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]\n\u001b[0m\u001b[1;32m   1675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/TF-1.3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_import_meta_graph_with_return_elements\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, return_elements, **kwargs)\u001b[0m\n\u001b[1;32m   1694\u001b[0m           \u001b[0mimport_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimport_scope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1695\u001b[0m           \u001b[0mreturn_elements\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_elements\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1696\u001b[0;31m           **kwargs))\n\u001b[0m\u001b[1;32m   1697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1698\u001b[0m   saver = _create_saver_from_imported_meta_graph(\n",
      "\u001b[0;32m~/venvs/TF-1.3/lib/python3.6/site-packages/tensorflow/python/framework/meta_graph.py\u001b[0m in \u001b[0;36mimport_scoped_meta_graph_with_return_elements\u001b[0;34m(meta_graph_or_file, clear_devices, graph, import_scope, input_map, unbound_inputs_col_name, restore_collections_predicate, return_elements)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0minput_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0mproducer_op_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproducer_op_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m         return_elements=return_elements)\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;31m# Restores all the other collections.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/TF-1.3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                 instructions)\n\u001b[0;32m--> 488\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    490\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/venvs/TF-1.3/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\u001b[0m in \u001b[0;36mimport_graph_def\u001b[0;34m(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\u001b[0m\n\u001b[1;32m    440\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m     \u001b[0m_ProcessNewOps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m   \u001b[0;31m# Treat input mappings that don't appear in the graph as an error, because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/TF-1.3/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\u001b[0m in \u001b[0;36m_ProcessNewOps\u001b[0;34m(graph)\u001b[0m\n\u001b[1;32m    232\u001b[0m   \u001b[0mcolocation_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mnew_op\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_new_tf_operations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_devices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m     \u001b[0moriginal_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0mnew_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/TF-1.3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_add_new_tf_operations\u001b[0;34m(self, compute_devices)\u001b[0m\n\u001b[1;32m   3438\u001b[0m     new_ops = [\n\u001b[1;32m   3439\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_from_tf_operation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_devices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3440\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mc_op\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_tf_operations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3441\u001b[0m     ]\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/TF-1.3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3438\u001b[0m     new_ops = [\n\u001b[1;32m   3439\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_from_tf_operation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_devices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3440\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mc_op\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_tf_operations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3441\u001b[0m     ]\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/TF-1.3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_from_tf_operation\u001b[0;34m(self, c_op, compute_device)\u001b[0m\n\u001b[1;32m   3297\u001b[0m     \"\"\"\n\u001b[1;32m   3298\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_not_finalized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3299\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3300\u001b[0m     \u001b[0;31m# If a name_scope was created with ret.name but no nodes were created in it,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3301\u001b[0m     \u001b[0;31m# the name will still appear in _names_in_use even though the name hasn't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/TF-1.3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1802\u001b[0m     ]\n\u001b[1;32m   1803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1804\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1806\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venvs/TF-1.3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_add_op\u001b[0;34m(self, op)\u001b[0m\n\u001b[1;32m   2941\u001b[0m                          \"is already used\" % op.name)\n\u001b[1;32m   2942\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2943\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2944\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = list(read.read_data_iterator('dataset/gw_extractions_no_rep_no_fin.pickle'))\n",
    "# train_data = dataset[:int(0.8 * len(features))]\n",
    "val_data = dataset[int(0.8 * len(features)):int(0.9*len(features))]\n",
    "\n",
    "for i, ec_dict in zip(range(10), val_data):\n",
    "    check_dataset = [ec_dict]\n",
    "    \n",
    "    predict_set = list(prepare_data.tokenize_if_small_enough(check_dataset, sentences=False))\n",
    "    predict_input_fn = input_builder.input_fn_builder(\n",
    "        features=predict_set,\n",
    "        seq_length=MAX_SEQ_LENGTH,\n",
    "        is_training=False,\n",
    "        drop_remainder=False,\n",
    "        candidates=5)\n",
    "\n",
    "    predictions = estimator.predict(input_fn=predict_input_fn,yield_single_examples=False)\n",
    "    for (i, prediction) in enumerate(predictions):\n",
    "        arr = prediction['probabilities'][0]\n",
    "        prob = [np.exp(arr[0]),np.exp(arr[1]),np.exp(arr[2]),np.exp(arr[3]),np.exp(arr[4])]\n",
    "        total = np.sum(prob)\n",
    "        max= np.max(prob)\n",
    "        print(\"SENTENCE EVENT CHAIN\", ec_dict['sentences'],\"\\n\")\n",
    "        print(\"TRIPLE EVENT CHAIN\", ec_dict['triples'],\"\\n\")\n",
    "        print(\"CANDIDATES\", ec_dict['candidates'],\"\\n\")\n",
    "        print(\"ENTITY\", ec_dict['entity'],\"\\n\")\n",
    "        print(\"CORRECT\", ec_dict['correct'] + 1,\"\\n\")\n",
    "        print(\"PREDICTION\", prediction['labels'] + 1, \"with Probability\", np.max(prob),\"\\n\",\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djjindal/venvs/TF-1.3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: generator 'read_data_iterator' raised StopIteration\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 09:41:50.397418 140409792354048 tf_logging.py:115] Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 09:41:53.753287 140409792354048 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 09:41:54.375032 140409792354048 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 09:41:55.391087 140409792354048 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 09:41:56.464885 140409792354048 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 09:41:57.176843 140409792354048 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 09:41:57.345880 140409792354048 tf_logging.py:115] Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 09:41:57.837945 140409792354048 tf_logging.py:115] Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from output_triple/model.ckpt-4800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 09:41:57.842859 140409792354048 tf_logging.py:115] Restoring parameters from output_triple/model.ckpt-4800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 09:41:59.480309 140409792354048 tf_logging.py:115] Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 09:41:59.612791 140409792354048 tf_logging.py:115] Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "dataset = list(read.read_data_iterator('dataset/gw_extractions_no_rep_no_fin.pickle'))\n",
    "train_data = dataset[:int(0.8 * len(features))]\n",
    "val_data = dataset[int(0.8 * len(features)):]\n",
    "\n",
    "check_dataset = []\n",
    "\n",
    "for i, ec_dict in zip(range(1000), val_data):\n",
    "    check_dataset.append(ec_dict)\n",
    "    \n",
    "    \n",
    "predict_set = list(prepare_data.tokenize_if_small_enough(check_dataset,sentences=True,no_context=False))\n",
    "predict_input_fn = input_builder.input_fn_builder(\n",
    "    features=predict_set,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False,\n",
    "    candidates=5)\n",
    "\n",
    "predictions = estimator.predict(input_fn=predict_input_fn,yield_single_examples=False)\n",
    "predictions_list = list(predictions)\n",
    "for (i, prediction) in enumerate(predictions):\n",
    "    arr = prediction['probabilities'][0]\n",
    "    prob = [np.exp(arr[0]),np.exp(arr[1]),np.exp(arr[2]),np.exp(arr[3]),np.exp(arr[4])]\n",
    "    total = np.sum(prob)\n",
    "    max= np.max(prob)\n",
    "    ec_dict = check_dataset[i]\n",
    "    print(\"SENTENCE EVENT CHAIN\", ec_dict['sentences'],\"\\n\")\n",
    "    print(\"TRIPLE EVENT CHAIN\", ec_dict['triples'],\"\\n\")\n",
    "    print(\"CANDIDATES\", ec_dict['candidates'],\"\\n\")\n",
    "    print(\"ENTITY\", ec_dict['entity'],\"\\n\")\n",
    "    print(prob)\n",
    "    print(\"CORRECT\", ec_dict['correct'] + 1,\"\\n\")\n",
    "    print(\"PREDICTION\", prediction['labels']+1, \"with Probability\", np.max(prob),\"\\n\",\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "19\n",
      "49\n",
      "77\n",
      "118\n",
      "162\n",
      "220\n",
      "241\n",
      "298\n",
      "365\n",
      "533\n",
      "539\n",
      "555\n",
      "583\n",
      "615\n",
      "651\n",
      "681\n",
      "696\n",
      "727\n",
      "836\n",
      "841\n",
      "901\n",
      "933\n",
      "947\n"
     ]
    }
   ],
   "source": [
    "bad = []\n",
    "for i, c in enumerate(check_dataset):\n",
    "    r = list(prepare_data.tokenize_if_small_enough([c],sentences=True,no_context=False))\n",
    "    if not r:\n",
    "        print(i)\n",
    "        bad.append(i)\n",
    "good_gt = [c for i, c in enumerate(check_dataset) if i not in bad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame.from_dict({'predictions':predictions_list, 'dataset':good_gt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7530737704918032\n"
     ]
    }
   ],
   "source": [
    "pred_df['pred_label'] = pred_df.predictions.apply(lambda x: x['labels'] - 1)\n",
    "pred_df['gt_label'] = pred_df.dataset.apply(lambda x: x['correct'])\n",
    "pred_df['correct_pred'] = pred_df.apply(lambda s: s.pred_label == s.gt_label, axis=1)\n",
    "print(pred_df.correct_pred.sum()/len(pred_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_pickle(\"./triple1000_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ec_dict in check_dataset:\n",
    "    print(\"SENTENCE EVENT CHAIN\", ec_dict['sentences'],\"\\n\")\n",
    "    print(\"TRIPLE EVENT CHAIN\", ec_dict['triples'],\"\\n\")\n",
    "    print(\"CANDIDATES\", ec_dict['candidates'],\"\\n\")\n",
    "    print(\"ENTITY\", ec_dict['entity'],\"\\n\")\n",
    "    print(\"CORRECT\", ec_dict['correct'] + 1,\"\\n\")\n",
    "    print(\"PREDICTION\", prediction['labels'], \"with Probability\", np.max(prob),\"\\n\",\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
