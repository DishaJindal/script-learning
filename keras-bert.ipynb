{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from bert.tokenization import FullTokenizer\n",
    "from tqdm import tqdm_notebook\n",
    "from tensorflow.keras import backend as K\n",
    "import prepare_data\n",
    "from prepare_data import tokenizer\n",
    "import read\n",
    "from keras.utils import to_categorical\n",
    "# # Initialize session\n",
    "sess = tf.Session()\n",
    "\n",
    "# # Params for bert model and tokenization\n",
    "# # bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "# # max_seq_length = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "First, we load the sample data IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=True\n",
    "no_context=False\n",
    "neeg_dataset=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = read.read_data_iterator('gw_extractions.pickle')\n",
    "features = list(prepare_data.tokenize_if_small_enough(train_dataset, sentence, no_context, is_neeg=neeg_dataset))\n",
    "\n",
    "train_features = features[100:]\n",
    "val_features = features[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids = np.array([f.input_ids for f in train_features])\n",
    "train_input_masks = np.array([f.input_mask for f in train_features])\n",
    "train_segment_ids = np.array([f.segment_ids for f in train_features])\n",
    "train_labels = np.array([to_categorical(f.label_id - 1, num_classes=5) for f in train_features])\n",
    "\n",
    "\n",
    "val_input_ids = np.array([f.input_ids for f in val_features])\n",
    "val_input_masks = np.array([f.input_mask for f in val_features])\n",
    "val_segment_ids = np.array([f.segment_ids for f in val_features])\n",
    "val_labels = np.array([to_categorical(f.label_id - 1, num_classes=5) for f in val_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize\n",
    "\n",
    "Next, tokenize our text to create `input_ids`, `input_masks`, and `segment_ids`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.layers.Layer):\n",
    "    def __init__(self, n_fine_tune_layers=10, **kwargs):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            prepare_data.BERT_MODEL_HUB,\n",
    "            trainable=self.trainable,\n",
    "            name=\"{}_module\".format(self.name)\n",
    "        )\n",
    "\n",
    "        trainable_vars = self.bert.variables\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "\n",
    "        # Select how many layers to fine tune\n",
    "        trainable_vars = trainable_vars[-self.n_fine_tune_layers :]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "            \n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "            \"pooled_output\"\n",
    "        ]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBertLayer(BertLayer):\n",
    "    def call(self, inputs):\n",
    "        return [super(MultiBertLayer, self).call(ip) for ip in zip(*inputs)]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #(batch size, num_labels, max_seq_size)\n",
    "        return (input_shape[0], input_shape[1], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "def build_model(max_seq_length, num_labels, append_vector_len=0): \n",
    "    in_ids = tf.keras.layers.Input(shape=(num_labels, max_seq_length), name=\"input_ids\")\n",
    "    in_masks = tf.keras.layers.Input(shape=(num_labels, max_seq_length), name=\"input_masks\")\n",
    "    in_segments = tf.keras.layers.Input(shape=(num_labels, max_seq_length), name=\"segment_ids\")\n",
    "    \n",
    "    inputs = [in_ids, in_masks, in_segments]\n",
    "    if append_vector_len:\n",
    "        in_append = tf.keras.layers.Input(shape=(num_labels, append_vector_len), name=\"input_append\")\n",
    "        inputs.append(in_append)\n",
    "    \n",
    "    split_in_ids = [tf.keras.layers.Lambda(lambda x: x[:, i, :])(in_ids) for i in range(num_labels)]\n",
    "    split_in_masks = [tf.keras.layers.Lambda(lambda x: x[:, i, :])(in_masks) for i in range(num_labels)]\n",
    "    split_in_segments = [tf.keras.layers.Lambda(lambda x: x[:, i, :])(in_segments) for i in range(num_labels)]\n",
    "    \n",
    "    if append_vector_len:\n",
    "        split_in_append = [tf.keras.layers.Lambda(lambda x: x[:, i, :])(in_append) for i in range(num_labels)]\n",
    "       \n",
    "    bert_outputs = MultiBertLayer(n_fine_tune_layers=0)([split_in_ids, split_in_masks, split_in_segments])\n",
    "#     bert_outputs = [BertLayer(n_fine_tune_layers=0)([in_id, in_mask, in_segment])\n",
    "#                     for (in_id, in_mask, in_segment) in zip(split_in_ids, split_in_masks, split_in_segments)]\n",
    "    \n",
    "    if append_vector_len:\n",
    "        augmented_outputs = [tf.keras.layers.Concatenate(axis=1)([bo, ia]) for (bo, ia) in zip(bert_outputs, in_append)]\n",
    "    else:\n",
    "        augmented_outputs = bert_outputs\n",
    "    concat_output = tf.keras.layers.Concatenate(axis=1)(augmented_outputs)\n",
    "    dense = tf.keras.layers.Dense(256, activation='relu')(concat_output)\n",
    "    pred = tf.keras.layers.Dense(num_labels, activation='softmax')(dense)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=pred)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(prepare_data.MAX_SEQ_LENGTH, num_labels=5)\n",
    "\n",
    "# Instantiate variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_vars(sess)\n",
    "\n",
    "model.fit(\n",
    "    [train_input_ids, train_input_masks, train_segment_ids], \n",
    "    train_labels,\n",
    "    validation_data=([val_input_ids, val_input_masks, val_segment_ids], val_labels),\n",
    "    epochs=1,\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('BertModel.h5')\n",
    "pre_save_preds = model.predict([test_input_ids[0:100], \n",
    "                                test_input_masks[0:100], \n",
    "                                test_segment_ids[0:100]]\n",
    "                              ) # predictions before we clear and reload model\n",
    "\n",
    "# Clear and load model\n",
    "model = None\n",
    "model = build_model(max_seq_length)\n",
    "initialize_vars(sess)\n",
    "model.load_weights('BertModel.h5')\n",
    "\n",
    "post_save_preds = model.predict([test_input_ids[0:100], \n",
    "                                test_input_masks[0:100], \n",
    "                                test_segment_ids[0:100]]\n",
    "                              ) # predictions after we clear and reload model\n",
    "all(pre_save_preds == post_save_preds) # Are they the same?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
