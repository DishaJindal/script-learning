{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0513 23:50:56.102522 140530192520960 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0513 23:50:57.735626 140530192520960 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0513 23:50:59.733575 140530192520960 saver.py:1483] Saver not created because there are no variables in the graph to restore\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from bert.tokenization import FullTokenizer\n",
    "from tqdm import tqdm_notebook\n",
    "from tensorflow.keras import backend as K\n",
    "import prepare_data\n",
    "from prepare_data import tokenizer, tokenize_if_small_enough\n",
    "import read\n",
    "from keras.utils import to_categorical\n",
    "# # Initialize session\n",
    "sess = tf.Session()\n",
    "\n",
    "# # Params for bert model and tokenization\n",
    "# # bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "# # max_seq_length = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "First, we load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=True\n",
    "no_context=False\n",
    "neeg_dataset=False\n",
    "conceptnet=True\n",
    "input_size=10000\n",
    "logs = './logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = read.read_data_iterator('dataset/gw_extractions_enriched.pickle')\n",
    "features = list(tokenize_if_small_enough(train_dataset,\n",
    "                                         sentence, no_context,\n",
    "                                         is_neeg=neeg_dataset,\n",
    "                                         conceptnet=conceptnet,\n",
    "                                         input_size=100))\n",
    "sample_size = len(features)\n",
    "training_pct = 0.8\n",
    "val_pct = 0.1\n",
    "test_pct = 0.1\n",
    "train_set_size = int(sample_size * training_pct)\n",
    "val_set_size = int(sample_size * val_pct)\n",
    "test_set_size = sample_size - train_set_size - val_set_size\n",
    "\n",
    "train_features = features[:train_set_size]\n",
    "val_features = features[train_set_size:train_set_size+val_set_size]\n",
    "test_features = features[train_set_size+val_set_size:train_set_size+val_set_size+test_set_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['is_real_example', 'label_id', 'input_mask', 'segment_ids', 'augmenting_vectors', 'input_ids'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0].__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize\n",
    "\n",
    "Next, tokenize our text to create `input_ids`, `input_masks`, and `segment_ids`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_max_of_max(ls_ls):\n",
    "    inner_max_pad = max([max([np.array(t).shape for t in twe], key=lambda x:x[0])[0] \n",
    "                   for twe in train_word_events])\n",
    "    inner_maxed = [np.array([np.pad(np.array(a), ((0, inner_max_pad-np.array(a).shape[0]), (0, 0)), mode='constant') for a in ls]) for ls in ls_ls]\n",
    "    outer_max = max((e.shape[0] for e in inner_maxed))\n",
    "    return np.array([np.pad(e, ((0, outer_max-e.shape[0]), (0, 0), (0, 0)), mode='constant') for e in inner_maxed])\n",
    "    \n",
    "def pad_to_max(ls):\n",
    "    ls = [np.array(e) for e in ls]\n",
    "    max_pad = max((e.shape[0] for e in ls))\n",
    "    return np.array([np.pad(e, ((0, max_pad-e.shape[0]), (0, 0)), mode='constant') for e in ls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids = np.array([f.input_ids for f in train_features])\n",
    "train_input_masks = np.array([f.input_mask for f in train_features])\n",
    "train_segment_ids = np.array([f.segment_ids for f in train_features])\n",
    "train_labels = np.array([to_categorical(f.label_id - 1, num_classes=5) for f in train_features])\n",
    "train_word_events = pad_to_max_of_max([f.event_concept_vectors for f in train_features])\n",
    "train_word_candidates = pad_to_max_of_max([f.candidate_concept_vectors for f in train_features])\n",
    "train_sent_events = pad_to_max([[np.concatenate([f.event_sentence_pos[i], f.event_sentence_dep[i]]) \n",
    "                      for i in range(len(f.event_sentence_pos))] \n",
    "                     for f in train_features])\n",
    "train_sent_candidates = np.zeros((train_sent_events.shape[0], 5, 1, train_sent_events.shape[-1]))\n",
    "\n",
    "\n",
    "val_input_ids = np.array([f.input_ids for f in val_features])\n",
    "val_input_masks = np.array([f.input_mask for f in val_features])\n",
    "val_segment_ids = np.array([f.segment_ids for f in val_features])\n",
    "val_labels = np.array([to_categorical(f.label_id - 1, num_classes=5) for f in val_features])\n",
    "val_word_events = pad_to_max_of_max([f.event_concept_vectors for f in val_features])\n",
    "val_word_candidates = pad_to_max_of_max([f.candidate_concept_vectors for f in val_features])\n",
    "val_sent_events = pad_to_max([[np.concatenate([f.event_sentence_pos[i], f.event_sentence_dep[i]]) \n",
    "                      for i in range(len(f.event_sentence_pos))] \n",
    "                     for f in val_features])\n",
    "val_sent_candidates = np.zeros((val_sent_events.shape[0], 5, 1, val_sent_events.shape[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(val_sent_candidates[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.layers.Layer):\n",
    "    def __init__(self, n_fine_tune_layers=10, **kwargs):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            prepare_data.BERT_MODEL_HUB,\n",
    "            trainable=self.trainable,\n",
    "            name=\"{}_module\".format(self.name)\n",
    "        )\n",
    "        \n",
    "        trainable_vars = self.bert.variables\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "\n",
    "        # Select how many layers to fine tune\n",
    "        trainable_vars = trainable_vars[-self.n_fine_tune_layers :]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "            \n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "            \"pooled_output\"\n",
    "        ]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBertLayer(BertLayer):\n",
    "    def call(self, inputs):\n",
    "        return [super(MultiBertLayer, self).call(ip) for ip in zip(*inputs)]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #(batch size, num_labels, max_seq_size)\n",
    "        return (input_shape[0], input_shape[1], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "def build_model(max_seq_length, num_labels, word_vec_len=0, sentence_vec_len=0, num_sents=5): \n",
    "    \n",
    "    #Inputs\n",
    "    in_ids = layers.Input(shape=(num_labels, max_seq_length), name=\"input_ids\")\n",
    "    in_masks = layers.Input(shape=(num_labels, max_seq_length), name=\"input_masks\")\n",
    "    in_segments = layers.Input(shape=(num_labels, max_seq_length), name=\"segment_ids\")\n",
    "    inputs = [in_ids, in_masks, in_segments]\n",
    "    \n",
    "    if word_vec_len:\n",
    "        in_candidates_words = layers.Input(shape=(num_labels, None, word_vec_len), name=\"input_candidates_words\")\n",
    "        in_events_words = layers.Input(shape=(num_sents, None, word_vec_len), name=\"input_event_words\")\n",
    "        inputs.extend([in_candidates_words, in_events_words])\n",
    "        \n",
    "    if sentence_vec_len:\n",
    "        in_candidates_sentences = layers.Input(shape=(num_labels, 1, sentence_vec_len), name=\"input_candidates_sentences\")\n",
    "        in_events_sentences = layers.Input(shape=(num_sents, sentence_vec_len), name=\"input_events_sentences\")\n",
    "        inputs.extend([in_candidates_sentences, in_events_sentences])\n",
    "    \n",
    "    \n",
    "    #Split inputs if they should be operated on individually\n",
    "    split_in_ids = [layers.Lambda(lambda x: x[:, i, :])(in_ids) for i in range(num_labels)]\n",
    "    split_in_masks = [layers.Lambda(lambda x: x[:, i, :])(in_masks) for i in range(num_labels)]\n",
    "    split_in_segments = [layers.Lambda(lambda x: x[:, i, :])(in_segments) for i in range(num_labels)]\n",
    "    \n",
    "    if word_vec_len:\n",
    "        #Split by candidate\n",
    "        split_in_candidates_words = [layers.Lambda(lambda x: x[:, i, :, :])(in_candidates_words) for i in range(num_labels)]\n",
    "        #Split by sentence\n",
    "        split_in_events_words = [layers.Lambda(lambda x: x[:, i, :, :])(in_events_words) for i in range(num_sents)]\n",
    "        \n",
    "    if sentence_vec_len:\n",
    "        split_in_candidates_sentences = [layers.Lambda(lambda x: x[:, i, :])(in_candidates_sentences) \n",
    "                                         for i in range(num_labels)]\n",
    "    \n",
    "    \n",
    "    #Bert\n",
    "    bert_outputs = MultiBertLayer(n_fine_tune_layers=0)([split_in_ids, split_in_masks, split_in_segments])\n",
    "    \n",
    "    \n",
    "    #Autoencoders to convert word and sentence embeddings into fixed vector embeddings\n",
    "    \n",
    "    \n",
    "    word_to_sentence_autoencoder = layers.LSTM(sentence_vec_len if sentence_vec_len else 300,\n",
    "                                               name=\"word_to_sword_to_sentence_autoencoder\")\n",
    "    sentences_to_vec_autoencoder = layers.LSTM(300, name='sentences_to_vec_autoencoder')\n",
    "    \n",
    "    if word_vec_len:\n",
    "        autoencoded_word_event_sentence_vectors = [\n",
    "            layers.Lambda(lambda x: K.expand_dims(word_to_sentence_autoencoder(x), 1))(ew) \n",
    "            for ew in split_in_events_words]\n",
    "#         autoencoded_candidates = [\n",
    "#             word_to_sentence_autoencoder(layers.Lambda(lambda x: K.expand_dims(x, 1))(cw))\n",
    "#             for cw in split_in_candidates_words]\n",
    "    \n",
    "        autoencoded_candidates = [\n",
    "            layers.Lambda(lambda x: K.expand_dims(x, 1))(word_to_sentence_autoencoder(cw))\n",
    "            for cw in split_in_candidates_words\n",
    "        ]\n",
    "        word_event_candidate_autoencoded = [\n",
    "            layers.Concatenate(axis=1)(autoencoded_word_event_sentence_vectors + [ac]) \n",
    "            for ac in autoencoded_candidates]\n",
    "\n",
    "        autencoded_chains_from_words = [sentences_to_vec_autoencoder(wec) \n",
    "                                        for wec in word_event_candidate_autoencoded]\n",
    "    if sentence_vec_len:\n",
    "        sentence_candidate_vectors = [\n",
    "            layers.Concatenate(axis=1)([in_events_sentences, ics]) \n",
    "            for ics in split_in_candidates_sentences]\n",
    "        autencoded_chains_from_sents = [sentences_to_vec_autoencoder(scv) \n",
    "                                        for scv in sentence_candidate_vectors]\n",
    "    \n",
    "    if word_vec_len and sentence_vec_len:\n",
    "        enhancing_vectors = [layers.Concatenate(axis=1)([wv, sv])\n",
    "            for (wv, sv) in \n",
    "            zip(autencoded_chains_from_words, \n",
    "                autencoded_chains_from_sents)]\n",
    "    elif word_vec_len:\n",
    "        enhancing_vectors = autencoded_chains_from_words\n",
    "    elif sentence_vec_len:\n",
    "        enhancing_vectors = autencoded_chains_from_sents\n",
    "        \n",
    "        \n",
    "    #Combine Bert and autoencoder embeddings (if provided)\n",
    "    if word_vec_len or sentence_vec_len:\n",
    "        augmented_outputs = [layers.Concatenate(axis=1)([bo, ev]) for (bo, ev) in zip(bert_outputs, enhancing_vectors)]\n",
    "    else:\n",
    "        augmented_outputs = bert_outputs\n",
    "    concat_output = layers.Concatenate(axis=1)(augmented_outputs)\n",
    "    \n",
    "    #Single Hidden Layer and classification\n",
    "    dense = layers.Dense(256, activation='relu')(concat_output)\n",
    "    pred = layers.Dense(num_labels, activation='softmax')(dense)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=pred)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    print(inputs)\n",
    "    return model\n",
    "\n",
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0514 00:37:25.743511 140530192520960 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0514 00:37:27.597175 140530192520960 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0514 00:37:28.714873 140530192520960 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0514 00:37:29.841170 140530192520960 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0514 00:37:30.996720 140530192520960 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0514 00:37:32.771093 140530192520960 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_candidates_words (InputLa (None, 5, None, 300) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_event_words (InputLayer)  (None, 6, None, 300) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_90 (Lambda)              (None, None, 300)    0           input_candidates_words[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_91 (Lambda)              (None, None, 300)    0           input_candidates_words[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_92 (Lambda)              (None, None, 300)    0           input_candidates_words[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_93 (Lambda)              (None, None, 300)    0           input_candidates_words[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_94 (Lambda)              (None, None, 300)    0           input_candidates_words[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_95 (Lambda)              (None, None, 300)    0           input_event_words[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_96 (Lambda)              (None, None, 300)    0           input_event_words[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_97 (Lambda)              (None, None, 300)    0           input_event_words[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_98 (Lambda)              (None, None, 300)    0           input_event_words[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_99 (Lambda)              (None, None, 300)    0           input_event_words[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_100 (Lambda)             (None, None, 300)    0           input_event_words[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "word_to_sword_to_sentence_autoe (None, 169)          317720      lambda_90[0][0]                  \n",
      "                                                                 lambda_91[0][0]                  \n",
      "                                                                 lambda_92[0][0]                  \n",
      "                                                                 lambda_93[0][0]                  \n",
      "                                                                 lambda_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_candidates_sentences (Inp (None, 5, 1, 169)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_106 (Lambda)             (None, 1, 169)       0           lambda_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_107 (Lambda)             (None, 1, 169)       0           lambda_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_108 (Lambda)             (None, 1, 169)       0           lambda_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_109 (Lambda)             (None, 1, 169)       0           lambda_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_110 (Lambda)             (None, 1, 169)       0           lambda_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_111 (Lambda)             (None, 1, 169)       0           lambda_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_112 (Lambda)             (None, 1, 169)       0           word_to_sword_to_sentence_autoenc\n",
      "__________________________________________________________________________________________________\n",
      "input_events_sentences (InputLa (None, 6, 169)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_101 (Lambda)             (None, 1, 169)       0           input_candidates_sentences[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "lambda_113 (Lambda)             (None, 1, 169)       0           word_to_sword_to_sentence_autoenc\n",
      "__________________________________________________________________________________________________\n",
      "lambda_102 (Lambda)             (None, 1, 169)       0           input_candidates_sentences[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "lambda_114 (Lambda)             (None, 1, 169)       0           word_to_sword_to_sentence_autoenc\n",
      "__________________________________________________________________________________________________\n",
      "lambda_103 (Lambda)             (None, 1, 169)       0           input_candidates_sentences[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "lambda_115 (Lambda)             (None, 1, 169)       0           word_to_sword_to_sentence_autoenc\n",
      "__________________________________________________________________________________________________\n",
      "lambda_104 (Lambda)             (None, 1, 169)       0           input_candidates_sentences[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "lambda_116 (Lambda)             (None, 1, 169)       0           word_to_sword_to_sentence_autoenc\n",
      "__________________________________________________________________________________________________\n",
      "lambda_105 (Lambda)             (None, 1, 169)       0           input_candidates_sentences[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "input_ids (InputLayer)          (None, 5, 128)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        (None, 5, 128)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        (None, 5, 128)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_32 (Concatenate)    (None, 7, 169)       0           lambda_106[0][0]                 \n",
      "                                                                 lambda_107[0][0]                 \n",
      "                                                                 lambda_108[0][0]                 \n",
      "                                                                 lambda_109[0][0]                 \n",
      "                                                                 lambda_110[0][0]                 \n",
      "                                                                 lambda_111[0][0]                 \n",
      "                                                                 lambda_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_37 (Concatenate)    (None, 7, 169)       0           input_events_sentences[0][0]     \n",
      "                                                                 lambda_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_33 (Concatenate)    (None, 7, 169)       0           lambda_106[0][0]                 \n",
      "                                                                 lambda_107[0][0]                 \n",
      "                                                                 lambda_108[0][0]                 \n",
      "                                                                 lambda_109[0][0]                 \n",
      "                                                                 lambda_110[0][0]                 \n",
      "                                                                 lambda_111[0][0]                 \n",
      "                                                                 lambda_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_38 (Concatenate)    (None, 7, 169)       0           input_events_sentences[0][0]     \n",
      "                                                                 lambda_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)    (None, 7, 169)       0           lambda_106[0][0]                 \n",
      "                                                                 lambda_107[0][0]                 \n",
      "                                                                 lambda_108[0][0]                 \n",
      "                                                                 lambda_109[0][0]                 \n",
      "                                                                 lambda_110[0][0]                 \n",
      "                                                                 lambda_111[0][0]                 \n",
      "                                                                 lambda_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_39 (Concatenate)    (None, 7, 169)       0           input_events_sentences[0][0]     \n",
      "                                                                 lambda_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)    (None, 7, 169)       0           lambda_106[0][0]                 \n",
      "                                                                 lambda_107[0][0]                 \n",
      "                                                                 lambda_108[0][0]                 \n",
      "                                                                 lambda_109[0][0]                 \n",
      "                                                                 lambda_110[0][0]                 \n",
      "                                                                 lambda_111[0][0]                 \n",
      "                                                                 lambda_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_40 (Concatenate)    (None, 7, 169)       0           input_events_sentences[0][0]     \n",
      "                                                                 lambda_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_36 (Concatenate)    (None, 7, 169)       0           lambda_106[0][0]                 \n",
      "                                                                 lambda_107[0][0]                 \n",
      "                                                                 lambda_108[0][0]                 \n",
      "                                                                 lambda_109[0][0]                 \n",
      "                                                                 lambda_110[0][0]                 \n",
      "                                                                 lambda_111[0][0]                 \n",
      "                                                                 lambda_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_41 (Concatenate)    (None, 7, 169)       0           input_events_sentences[0][0]     \n",
      "                                                                 lambda_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_75 (Lambda)              (None, 128)          0           input_ids[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_76 (Lambda)              (None, 128)          0           input_ids[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_77 (Lambda)              (None, 128)          0           input_ids[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_78 (Lambda)              (None, 128)          0           input_ids[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_79 (Lambda)              (None, 128)          0           input_ids[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_80 (Lambda)              (None, 128)          0           input_masks[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_81 (Lambda)              (None, 128)          0           input_masks[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_82 (Lambda)              (None, 128)          0           input_masks[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_83 (Lambda)              (None, 128)          0           input_masks[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_84 (Lambda)              (None, 128)          0           input_masks[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_85 (Lambda)              (None, 128)          0           segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_86 (Lambda)              (None, 128)          0           segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_87 (Lambda)              (None, 128)          0           segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_88 (Lambda)              (None, 128)          0           segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_89 (Lambda)              (None, 128)          0           segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sentences_to_vec_autoencoder (L (None, 300)          564000      concatenate_32[0][0]             \n",
      "                                                                 concatenate_33[0][0]             \n",
      "                                                                 concatenate_34[0][0]             \n",
      "                                                                 concatenate_35[0][0]             \n",
      "                                                                 concatenate_36[0][0]             \n",
      "                                                                 concatenate_37[0][0]             \n",
      "                                                                 concatenate_38[0][0]             \n",
      "                                                                 concatenate_39[0][0]             \n",
      "                                                                 concatenate_40[0][0]             \n",
      "                                                                 concatenate_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "multi_bert_layer_3 (MultiBertLa [(None, 768), (None, 110104890   lambda_75[0][0]                  \n",
      "                                                                 lambda_76[0][0]                  \n",
      "                                                                 lambda_77[0][0]                  \n",
      "                                                                 lambda_78[0][0]                  \n",
      "                                                                 lambda_79[0][0]                  \n",
      "                                                                 lambda_80[0][0]                  \n",
      "                                                                 lambda_81[0][0]                  \n",
      "                                                                 lambda_82[0][0]                  \n",
      "                                                                 lambda_83[0][0]                  \n",
      "                                                                 lambda_84[0][0]                  \n",
      "                                                                 lambda_85[0][0]                  \n",
      "                                                                 lambda_86[0][0]                  \n",
      "                                                                 lambda_87[0][0]                  \n",
      "                                                                 lambda_88[0][0]                  \n",
      "                                                                 lambda_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_42 (Concatenate)    (None, 600)          0           sentences_to_vec_autoencoder[0][0\n",
      "                                                                 sentences_to_vec_autoencoder[5][0\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_43 (Concatenate)    (None, 600)          0           sentences_to_vec_autoencoder[1][0\n",
      "                                                                 sentences_to_vec_autoencoder[6][0\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_44 (Concatenate)    (None, 600)          0           sentences_to_vec_autoencoder[2][0\n",
      "                                                                 sentences_to_vec_autoencoder[7][0\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_45 (Concatenate)    (None, 600)          0           sentences_to_vec_autoencoder[3][0\n",
      "                                                                 sentences_to_vec_autoencoder[8][0\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_46 (Concatenate)    (None, 600)          0           sentences_to_vec_autoencoder[4][0\n",
      "                                                                 sentences_to_vec_autoencoder[9][0\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_47 (Concatenate)    (None, 1368)         0           multi_bert_layer_3[0][0]         \n",
      "                                                                 concatenate_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_48 (Concatenate)    (None, 1368)         0           multi_bert_layer_3[0][1]         \n",
      "                                                                 concatenate_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_49 (Concatenate)    (None, 1368)         0           multi_bert_layer_3[0][2]         \n",
      "                                                                 concatenate_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_50 (Concatenate)    (None, 1368)         0           multi_bert_layer_3[0][3]         \n",
      "                                                                 concatenate_45[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_51 (Concatenate)    (None, 1368)         0           multi_bert_layer_3[0][4]         \n",
      "                                                                 concatenate_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_52 (Concatenate)    (None, 6840)         0           concatenate_47[0][0]             \n",
      "                                                                 concatenate_48[0][0]             \n",
      "                                                                 concatenate_49[0][0]             \n",
      "                                                                 concatenate_50[0][0]             \n",
      "                                                                 concatenate_51[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          1751296     concatenate_52[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 5)            1285        dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 112,739,191\n",
      "Trainable params: 112,116,541\n",
      "Non-trainable params: 622,650\n",
      "__________________________________________________________________________________________________\n",
      "[<tf.Tensor 'input_ids_2:0' shape=(?, 5, 128) dtype=float32>, <tf.Tensor 'input_masks_2:0' shape=(?, 5, 128) dtype=float32>, <tf.Tensor 'segment_ids_2:0' shape=(?, 5, 128) dtype=float32>, <tf.Tensor 'input_candidates_words_2:0' shape=(?, 5, ?, 300) dtype=float32>, <tf.Tensor 'input_event_words_2:0' shape=(?, 6, ?, 300) dtype=float32>, <tf.Tensor 'input_candidates_sentences_1:0' shape=(?, 5, 1, 169) dtype=float32>, <tf.Tensor 'input_events_sentences_1:0' shape=(?, 6, 169) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "model = build_model(prepare_data.MAX_SEQ_LENGTH, num_labels=5, word_vec_len=300, sentence_vec_len=169, num_sents=train_word_events.shape[1])\n",
    "\n",
    "# Instantiate variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_word_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 7, 169)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sent_events.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_events_sentences to have shape (6, 169) but got array with shape (7, 169)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-a4fbc1458803>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle)\u001b[0m\n\u001b[1;32m   2380\u001b[0m         \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2382\u001b[0;31m         exception_prefix='input')\n\u001b[0m\u001b[1;32m   2383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    360\u001b[0m                 \u001b[0;34m'Error when checking '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexception_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m                 ' but got array with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    363\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_events_sentences to have shape (6, 169) but got array with shape (7, 169)"
     ]
    }
   ],
   "source": [
    "initialize_vars(sess)\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(os.path.join(logs, '{epoch:02d}.h5')),\n",
    "    keras.callbacks.TensorBoard(log_dir=logs, update_freq=1000)\n",
    "]\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    [train_input_ids, train_input_masks, train_segment_ids, \n",
    "     train_word_candidates, train_word_events, train_sent_candidates, train_sent_events], \n",
    "    train_labels,\n",
    "    validation_data=([val_input_ids, val_input_masks, val_segment_ids, \n",
    "                      val_word_candidates, np.array(val_word_events), val_sent_candidates, val_sent_events], val_labels),\n",
    "    epochs=3,\n",
    "    batch_size=1,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('BertModel.h5')\n",
    "pre_save_preds = model.predict([test_input_ids[0:100], \n",
    "                                test_input_masks[0:100], \n",
    "                                test_segment_ids[0:100]]\n",
    "                              ) # predictions before we clear and reload model\n",
    "\n",
    "# Clear and load model\n",
    "model = None\n",
    "model = build_model(max_seq_length)\n",
    "initialize_vars(sess)\n",
    "model.load_weights('BertModel.h5')\n",
    "\n",
    "post_save_preds = model.predict([test_input_ids[0:100], \n",
    "                                test_input_masks[0:100], \n",
    "                                test_segment_ids[0:100]]\n",
    "                              ) # predictions after we clear and reload model\n",
    "all(pre_save_preds == post_save_preds) # Are they the same?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
