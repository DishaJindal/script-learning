{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import read\n",
    "import prepare_data\n",
    "import input_builder\n",
    "\n",
    "import importlib\n",
    "import model\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_SEQ_LENGTH = 128\n",
    "\n",
    "importlib.reload(input_builder)\n",
    "import os\n",
    "os.environ['TFHUB_CACHE_DIR'] = '/home/djjindal/bert/script-learning'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "model_dir = 'output_sentence'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<input_builder.InputFeatures object at 0x7f169f36f860>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7874"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = 'dataset/gw_extractions_no_rep_no_fin.pickle'\n",
    "train_dataset = read.read_data_iterator(dataset)\n",
    "features = list(prepare_data.tokenize_if_small_enough(train_dataset,sentences=True,no_context=False))\n",
    "train_set = features[:int(0.8 * len(features))]\n",
    "val_set = features[int(0.8 * len(features)):int(0.9*len(features))]\n",
    "print(train_set[0])\n",
    "len(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'output_sentence', '_tf_random_seed': None, '_save_summary_steps': 0, '_save_checkpoints_steps': 0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f17183b9cf8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 00:19:57.087816 139737371649792 tf_logging.py:115] Using config: {'_model_dir': 'output_sentence', '_tf_random_seed': None, '_save_summary_steps': 0, '_save_checkpoints_steps': 0, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f17183b9cf8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=model_dir,\n",
    "    save_summary_steps=0,\n",
    "    save_checkpoints_steps=0,\n",
    "    log_step_count_steps=100)\n",
    "\n",
    "model_fn = model.model_fn_builder(\n",
    "  num_labels=5,\n",
    "  learning_rate=LEARNING_RATE,\n",
    "  num_train_steps=1,\n",
    "  num_warmup_steps=1)\n",
    "\n",
    "train_test_input_fn = input_builder.input_fn_builder(\n",
    "    features=train_set,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False,\n",
    "    candidates=5)\n",
    "\n",
    "val_test_input_fn = input_builder.input_fn_builder(\n",
    "    features=val_set,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False,\n",
    "    candidates=5)\n",
    "\n",
    "estimator = tf.estimator.Estimator(\n",
    "  model_fn=model_fn,\n",
    "  config=run_config,\n",
    "  params={\"batch_size\": BATCH_SIZE})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICT FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def predict(sentenecs, triples, candidates, entity):\n",
    "    e_dict = dict()\n",
    "    check_dataset = []\n",
    "    e_dict['sentences'] = sentenecs\n",
    "    e_dict['triples'] = triples\n",
    "    e_dict['candidates'] = candidates\n",
    "    e_dict['correct'] = 4\n",
    "    e_dict['entity'] = entity\n",
    "    check_dataset = [e_dict]\n",
    "    predict_set = list(prepare_data.tokenize_if_small_enough(check_dataset, sentences=True))\n",
    "\n",
    "    predict_input_fn = input_builder.input_fn_builder(\n",
    "        features=predict_set,\n",
    "        seq_length=MAX_SEQ_LENGTH,\n",
    "        is_training=False,\n",
    "        drop_remainder=False,\n",
    "        candidates=5)\n",
    "    predictions = estimator.predict(input_fn=predict_input_fn,yield_single_examples=False)\n",
    "\n",
    "    for (i, prediction) in enumerate(predictions):\n",
    "        arr = prediction['probabilities'][0]\n",
    "        prob = [np.exp(arr[0]),np.exp(arr[1]),np.exp(arr[2]),np.exp(arr[3]),np.exp(arr[4])]\n",
    "        total = np.sum(prob)\n",
    "        max= np.max(prob)\n",
    "        ec_dict = check_dataset[i]\n",
    "        print(\"SENTENCE EVENT CHAIN\", ec_dict['sentences'],\"\\n\")\n",
    "        print(\"TRIPLE EVENT CHAIN\", ec_dict['triples'],\"\\n\")\n",
    "        print(\"CANDIDATES\", ec_dict['candidates'],\"\\n\")\n",
    "        print(\"ENTITY\", ec_dict['entity'],\"\\n\")\n",
    "        print(prob)\n",
    "        print(\"PREDICTION\", prediction['labels'], \"with Probability\", np.max(prob),\"\\n\",\"\\n\")\n",
    "        return prediction['labels']\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    print(text1.value, text2.value, text3.value)\n",
    "    print(predict(text1.value.split('), ('),[], text2.value.split('), ('), text3.value))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3298986155cc42ee95948a6e0ab391ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Event Chain')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf1f6b1527e4582b8fbbc13c9456bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Candidates')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f311ab5ea84c4f0c9776a67140264483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Entity')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14eebbaf09674e6fb060b707c32c8665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Predict', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import *\n",
    "from IPython.display import display\n",
    "from IPython.html import widgets\n",
    "text1 = widgets.Text(description=\"Event Chain\", width=200)\n",
    "text2 = widgets.Text(description=\"Candidates\", width=200)\n",
    "text3 = widgets.Text(description=\"Entity\", width=200)\n",
    "button = widgets.Button(description=\"Predict\")\n",
    "button.on_click(on_button_clicked)\n",
    "display(text1)\n",
    "display(text2)\n",
    "display(text3)\n",
    "display(button)\n",
    "\n",
    "# ['For the most part, critics\\nembraced the work, but the authors did not know if the book-buying\\npublic would; now, three months after its release in late\\nSeptember, the two have traveled to more than 30 cities to push\\ntheir book at lectures, talks, and book signings.', 'The authors\\ncreated a Web site.', \"They've done more than 40 print interviews.\"]\n",
    "# [(None, 'promoting', '``Marie Curie'), ('no way', 'set', None), ('the fall season', 'there is', None), (None, 'hired', 'radio'), ('James Carroll', 'makes', None)]\n",
    "\n",
    "# [('john','ordered',None),('john','ate','food')]\n",
    "# [('john','left',None),('john','stays',None)]\n",
    "\n",
    "# [('john','ordered',None),('john','paid',None),('john','ate','food')]\n",
    "# [('john','left',None),('john','stays',None)]\n",
    "\n",
    "# [(None, 'took', 'phone call'), (None, 'had', 'something'), (None, 'began', 'to squint')] \n",
    "# [(None, 'broadcast', '(the two previous record-holders'), ('the pennant', 'slammed', None), (None, \"n't imagine\", 'Maris'), (None, 'expressed', 'interest'), (None, 'turned', '68 _')] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/djjindal/venvs/TF-1.3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: generator 'read_data_iterator' raised StopIteration\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 00:23:14.995319 139737371649792 tf_logging.py:115] Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 00:23:18.440169 139737371649792 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 00:23:19.490723 139737371649792 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 00:23:20.595565 139737371649792 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 00:23:21.344037 139737371649792 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 00:23:22.545407 139737371649792 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 00:23:22.734081 139737371649792 tf_logging.py:115] Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 00:23:23.260912 139737371649792 tf_logging.py:115] Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from output_sentence/model.ckpt-4700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 00:23:23.265189 139737371649792 tf_logging.py:115] Restoring parameters from output_sentence/model.ckpt-4700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 00:23:24.798485 139737371649792 tf_logging.py:115] Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0422 00:23:24.937283 139737371649792 tf_logging.py:115] Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "dataset = list(read.read_data_iterator('dataset/gw_extractions_no_rep_no_fin.pickle'))\n",
    "train_data = dataset[:int(0.8 * len(features))]\n",
    "val_data = dataset[int(0.8 * len(features)):int(0.9*len(features))]\n",
    "\n",
    "check_dataset = []\n",
    "\n",
    "for i, ec_dict in zip(range(1000), val_data):\n",
    "    check_dataset.append(ec_dict)\n",
    "    \n",
    "predict_set = list(prepare_data.tokenize_if_small_enough(check_dataset,sentences=True,no_context=False))\n",
    "predict_input_fn = input_builder.input_fn_builder(\n",
    "    features=predict_set,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False,\n",
    "    candidates=5)\n",
    "\n",
    "predictions = estimator.predict(input_fn=predict_input_fn,yield_single_examples=False)\n",
    "predictions_list = list(predictions)\n",
    "for (i, prediction) in enumerate(predictions):\n",
    "    arr = prediction['probabilities'][0]\n",
    "    prob = [np.exp(arr[0]),np.exp(arr[1]),np.exp(arr[2]),np.exp(arr[3]),np.exp(arr[4])]\n",
    "    total = np.sum(prob)\n",
    "    max= np.max(prob)\n",
    "    ec_dict = check_dataset[i]\n",
    "    print(\"SENTENCE EVENT CHAIN\", ec_dict['sentences'],\"\\n\")\n",
    "    print(\"TRIPLE EVENT CHAIN\", ec_dict['triples'],\"\\n\")\n",
    "    print(\"CANDIDATES\", ec_dict['candidates'],\"\\n\")\n",
    "    print(\"ENTITY\", ec_dict['entity'],\"\\n\")\n",
    "    print(prob)\n",
    "    print(\"CORRECT\", ec_dict['correct'] + 1,\"\\n\")\n",
    "    print(\"PREDICTION\", prediction['labels']+1, \"with Probability\", np.max(prob),\"\\n\",\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "19\n",
      "49\n",
      "77\n"
     ]
    }
   ],
   "source": [
    "bad = []\n",
    "for i, c in enumerate(check_dataset):\n",
    "    r = list(prepare_data.tokenize_if_small_enough([c],sentences=True,no_context=False))\n",
    "    if not r:\n",
    "        print(i)\n",
    "        bad.append(i)\n",
    "good_gt = [c for i, c in enumerate(check_dataset) if i not in bad]\n",
    "pred_df = pd.DataFrame.from_dict({'predictions':predictions_list, 'dataset':good_gt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7395833333333334\n"
     ]
    }
   ],
   "source": [
    "pred_df['pred_label'] = pred_df.predictions.apply(lambda x: x['labels'] - 1)\n",
    "pred_df['gt_label'] = pred_df.dataset.apply(lambda x: x['correct'])\n",
    "pred_df['correct_pred'] = pred_df.apply(lambda s: s.pred_label == s.gt_label, axis=1)\n",
    "print(pred_df.correct_pred.sum()/len(pred_df.index))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
